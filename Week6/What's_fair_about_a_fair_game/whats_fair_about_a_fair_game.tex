\documentclass[UTF8]{article}

\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{theorem}
% \usepackage[dutch]{babel}
\usepackage{amsmath, amssymb}
\usepackage{import}
\usepackage{pdfpages}
\usepackage{bm}
\usepackage{transparent}
\usepackage{xcolor}
\usepackage{enumerate}
\usepackage{setspace} 
%\usepackage{ebgaramond}
%\fontfamily{ebgaramond}
% ------------- coding style setting --------------%
%\usepackage{libertine}
\usepackage{listings}
\usepackage{enumitem}
\setlist{nosep}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

% ----------------- geometry and fancy head -----------

\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=3cm,bottom=3cm}
\usepackage[many]{tcolorbox}
\tcbuselibrary{skins, breakable, theorems}

\usepackage{fancyhdr}
\usepackage{syntonly} % dubugging
% \syntaxonly
\fancypagestyle{mainFancy}{
    \fancyhf{}
    %\renewcommand\headrulewidth{0pt}       % 页眉横线
    %\renewcommand\footrulewidth{0pt}
    
    \fancyhead[L]{Probability Theory}       % 页眉章标题
    \fancyhead[R]{Assignment}         % 页眉文章题目
    \fancyfoot[C]{\thepage}                 % 页眉编号
}
\pagestyle{mainFancy}


% --------------- environment setting ------------------

\newtheorem{thm}{Theorem}
\newtheorem{pro}{Problem}
\newtheorem{lemma}{Lemma}
\newtheorem{defi}{Definition}
\newtheorem{li}{Example}
\newenvironment{proof}{\paragraph{Proof:}}{\hfill$\square$}
\newenvironment{jie}{\paragraph{Show:}}{\hfill$\square$}



\tcolorboxenvironment{pro}{
  enhanced,
  borderline={0.4pt}{0.4pt}{black},
  boxrule=0.4pt,
  colback=white,
  coltitle=black,
  sharp corners,
}
\tcolorboxenvironment{thm}{
  enhanced,
  borderline={0.4pt}{0.4pt}{black},
  boxrule=0.4pt,
  colback=white,
  coltitle=black,
  sharp corners,
}
\tcolorboxenvironment{lemma}{
  enhanced,
  borderline={0.4pt}{0.4pt}{black},
  boxrule=0.4pt,
  colback=white,
  coltitle=black,
  sharp corners,
}
\tcolorboxenvironment{defi}{
  enhanced,
  borderline={0.4pt}{0.4pt}{black},
  boxrule=0.4pt,
  colback=white,
  coltitle=black,
  sharp corners,
}

% ----------------- macros and command -----------------
\usepackage{stmaryrd} 
\newcommand\contra{\scalebox{1.5}{$\lightning$}}
\definecolor{correct}{HTML}{009900}
\newcommand\correct[2]{\ensuremath{\:}{\color{red}{#1}}\ensuremath{\to }{\color{correct}{#2}}\ensuremath{\:}}
\newcommand\green[1]{{\color{correct}{#1}}}

% horizontal rule
\newcommand\hr{
		    \noindent\rule[0.5ex]{\linewidth}{0.5pt}
	}
\def\mf(#1){\mathfrak{#1}} 
\def\setn(#1,#2){\left\{#1_1,#1_2,\cdots, #1_#2 \right\}  }


\let\implies\Rightarrow
\let\impliedby\Leftarrow
\let\iff\Leftrightarrow
\let\ldots\cdots


\newcommand\dif{\,\mathrm{d}}
\newcommand\e{\,\mathrm{e}}
\newcommand\R{\,\mathbb{R}}
\newcommand\Q{\,\mathbb{Q}}
\newcommand\C{\,\mathbb{C}}
\newcommand\N{\,\mathbb{N}}
\newcommand\A{\,\mathbb{A}}
\newcommand\Z{\,\mathbb{Z}}
\newcommand\ep{\,\varepsilon}
\newcommand\F{\,\varphi}
\newcommand\T{\,\mathbb{T}}
\newcommand\HH{\,\mathbb{H}}
\author{
	yujie6@sjtu.edu.cn\footnote{Yujie Lu, ACM class 18, ID is 518030910111} \\
	dataisland@sjtu.edu.cn\footnote{Zheng Yu, ACM class 18, ID is 518030910437}
}

\linespread{1.5} \selectfont

\title{\textsc{What's Fair about a Fair Game?}}
\begin{document}
\maketitle
        
\begin{pro}[E4.7]
Let $X_1,X_2\ldots$ be independent RVs such that 
\[
X_{n} = \begin{cases}
		 n^2-1 \quad \text{ with probability } n^{-2} \\
		 -1 \quad \text{ with probability } 1- n^{-2}  
\end{cases}
.\]  
Prove that $\mathbb{E} \left( X_{n} \right) = 0, \forall n$. But that if $S_{n} = \sum_{i=1}^{n} X_{n}$, then 
\[
\frac{S_n}{n} \to  -1 , \quad \text{a.s.}
.\] 
\end{pro}
\paragraph{Proof 1:}
		By definition, it's obvious that $\mathbb{E} \left( X_{i} \right) = 0,\forall i$.
		Notice that 
		\[
				\frac{S_n}{n}+1 =  \frac{S_n + n}{n} = 
				\frac{1}{n}\left( \sum_{i=1}^{n} Y_{i} \right) 
		.\] 
		In which $Y_{i} = 1 + X_{i}$. Then 
		$\mathbb{E}\left( Y_{i} \right) = 1$. We only need to prove
		\[
		\frac{1}{n} \sum_{i=1}^{n} Y_{i}= \frac{1}{n}S'_{n} \to 0 \quad \text{a.s. }
		.\] 
		My first attempt is using \textbf{Chebyshev's Inequality}, 
		take $\phi=x^{\alpha }$, here 
		$\alpha $ is a constant we may decide later, we have 
		\[
				P\left( \frac{1}{n} S'_{n} > \ep \right) 
				\le \frac{\mathbb{E} \left( \left( S'_{n} \right) ^{\alpha } \right)  }{ \left( n\ep \right) ^{ \alpha  } }
		.\] 
		And we try to prove that $\mathbb{E} \left( \left( S'_{n} \right) ^{\alpha } \right) $ is really small, at least smaller that $n^{\alpha }$. 
However, it's impossible to prove the result this way, whatever $\alpha $ we choose.

		Which means this bound is too tight to use Chebyshev. Hence I try another
		method in the proof of \textbf{Chernoff Bounds}\footnote{For more details, 
refer to wiki \texttt{https://en.wikipedia.org/wiki/Chernoff\_bound} }, 
		\begin{thm}[Chernoff bound]
				Let $X_1,\ldots$ be independent Bernoulli random variables, 
				then 
				\[
						P\left( \frac{S_n}{n} \ge \mathbb{E}\left[ S_n \right] + \ep \right) \le \exp\left( -D\left( p+\ep \parallel p \right) n \right) 
				.\] 
		\end{thm}
The bound is exactly what we want to have. However, the method used in the theorem 
could not be applied to this problem directly.

		Finally I tried the technique used in the proof of weak law for triangular arrays, 
		Let $\overline{Y_{i}}= Y_{i} \cdot 1_{Y_{i} \le  n}$. Hence we have 
		\[
				\sum_{i=1}^{n} P\left( Y_{i} > n \right) = 
				\sum_{i^2 > n}^{n} \frac{1}{i^2} < \frac{1}{\sqrt{n} } - \frac{1}{n}
				\to  0
		.\]
		Plus 
		\[
		n^{-2} \sum_{i=1}^{n} \mathbb{E} \overline{Y_i}^2 < 
		\frac{1}{n^2} \cdot \left\lceil \sqrt{n}  \right\rceil \to  0 
		.\] 
		Let $\overline{S}_n = \sum_{i=1}^{n} \overline{Y_i}$, it follows 
		that $a_n = \mathbb{E} \left( \overline{S}_n \right) = \left\lfloor \sqrt{n}  \right\rfloor$. We now prove a stronger result: 
		\[
				\frac{ S'_n -a_n }{n} \to  0  
		.\] 
		Clearly
		\[
				P\left( \left| \frac{S_n-a_n}{n} \right| > \ep \right) \le 
				P\left( S'_n \neq \overline{S}_n \right) + 
				P \left( \left| \frac{\overline{S}_n-a_n}{n} \right| > \ep  \right) 
		.\] 
		To estimate the first term, notice that 
		\[
				P\left( S_n \neq \overline{S}_n \right) \le  
				P \left( \bigcup_{i=1}^{n} \{ \overline{Y}_i\neq Y_i\}   \right) 
				\le \sum_{i=1}^{n} P\left( Y_i > n \right) \to 0
		.\] 
		For the second term, we use \textbf{Chebyshev's inequality} which is a 
		common trick
		\[
				P \left( \left| \frac{\overline{S}_n-a_n}{n} \right| > \ep  \right) 
				\le  \frac{ \mathrm{var}\left( \overline{S}_n \right)   }{ n^2\ep^2}
				\le \frac{1}{n^2\ep^2} \cdot \sum_{i=1}^{n} \mathbb{E} \left( \overline{Y}_i \right) ^2 \to 0
		.\] 
		Therefore 
		\[
				\frac{S'_n }{n} - \frac{1}{\sqrt{n} } \to 0 
		.\] 
		As $\frac{1}{\sqrt{n} }\to 0$, our proof is completed.

		\noindent \textbf{Remark:}

		The construction of $\overline{Y}_i$ might be a bit tricky. But it's also somewhat natural. When I try the first 2 methods, I found that $n^2$ is just too big for 
		me to use some inequality tricks on the whole $Y_i,1\le i\le n$. 
		which prevents me estimating the size, that's why we need to do some truncation.

		And back to the title, the result of this problem reveals that even if the 
		expectation of profit in every term of a game you play is 0, which seams 
		fair. The average of your profit
		can be negative if you play it for too much terms. It tells us don't always take 
		risks in such games.
		\hfill$\square$

\paragraph{Proof 2:}
	Consider events $E_n = \{X_n = n^2-1\}$, we can know
	$$
	\sum_n P(E_n) = \sum_n P(X_n = n^2-1)=\sum_n \frac 1 {n^2} < \infty
	$$
	by BC1, get $P(E_n.i.o)=0$. So $\exists N$, $\forall n > N$, $s.t.$
	$$
	P(X_n = -1) = 1
	$$
	Because $X_1,X_2\ldots$ be independent RVs, so we can also know
	$$
	P(S_n \geq -n) = \prod _n P(X_n \geq -1) = 1 \Rightarrow P(\frac {S_n} n \geq -1) = 1
	$$
	and for $n > N$
	$$
	P(S_n - S_N = -n + N) = \prod _n P(X_n = -1) = 1
	$$
	Hence
	$$
	P(S_n \leq -n + N + \sum_{i=1}^N {i^2-1})=1
	$$
	And event $E_n^c = \{ X_n = -1 \}$,
	Let $F = \{ \lim_{n \rightarrow \infty} \frac {S_n} n = -1\}$. We can know $(E_n^c. e.v) \in F$。
	$$
	P(F) \geq P(E_n^c, ev) = 1
	$$
	Finally, we conlude
	$$
	\frac {S_n} n \rightarrow -1 (a.s.)
	$$
	\hfill$\square$
\end{document}
