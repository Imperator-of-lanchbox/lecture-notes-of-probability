% This is a template for lecture notes.
\documentclass[12pt]{article}
\usepackage{amssymb}
\usepackage[UTF8]{ctex}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{cite}
%\usepackage{CJK}
\usepackage[many]{tcolorbox}
%\tcbuselibrary{listingsutf8}
%\tcbuselibrary{skins, breakable, theorems, most}
%\geometry{a4paper,bottom = 3cm,left = 3cm, right = 3cm}
\CTEXoptions[today=old]
%for reference
\usepackage{hyperref}
\usepackage[capitalise]{cleveref}
\crefname{enumi}{}{}

\newtheoremstyle{mythm}{1.5ex plus 1ex minus .2ex}{1.5ex plus 1ex minus .2ex} 
    {}{\parindent}{\bfseries}{}{1em}{} 
\theoremstyle{mythm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem*{remark}{Remark}

%\newenvironment{proof}{\noindent \textbf{Proof:}}{$\Box$}

%to use newcommand for convenience
\newcommand\field{\mathbb{F}}
\newcommand\Real{\mathbb{R}}
\newcommand\Q{\mathbb{Q}}
\newcommand\Z{\mathbb{Z}}
\newcommand\complex{\mathbb{C}}
\newcommand\cc{\mathcal{C}}
\newcommand\uu{\mathcal{U}}
\newcommand\pp{\mathcal{P}}
\newcommand\ff{\mathcal{F}}
\renewcommand\refname{Reference}
\renewcommand{\proofname}{Proof}
\DeclareMathOperator{\range}{range}   

\title{Kullback–Leibler divergence $\ge$ 0}
\author{马浩博 518030910428}
\date{\today}
\begin{document}
\maketitle

Kullback–Leibler divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution. 
\section*{Definition}
For discrete probability distributions $P$ and $Q$ defined on the same probability space, ${\mathcal {X}}$, the Kullback–Leibler divergence from $Q$ to $P$ is defined to be
$$D_{\text{KL}}(P\parallel Q)=\sum _{x\in {\mathcal {X}}}P(x)\log \left({\frac {P(x)}{Q(x)}}\right)$$

For distributions $P$ and $Q$ of a continuous random variable, the Kullback–Leibler divergence is defined to be the integral:

$$D_{\text{KL}}(P\parallel Q)=\int _{-\infty }^{\infty }p(x)\log \left({\frac {p(x)}{q(x)}}\right)\,dx$$
where $p$ and $q$ denote the probability densities of $P$ and $Q$.

\section*{Applications}
Applications include characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference. In the simple case, a Kullback–Leibler divergence of 0 indicates that the two distributions in question are identical. In simplified terms, it is a measure of surprise, with diverse applications such as applied statistics, fluid mechanics, neuroscience and machine learning.

\begin{proof}
	We shall prove that $D_{KL}(P||Q) \ge 0$, here $D_{KL}$ means Kullback–Leibler divergence and $D(P||Q)=-\sum_x P(x)\ln \frac{Q(x)}{P(x)}$. 
	
	\begin{align}
	-D(P||Q)&= \sum_x P(x)\ln \frac{Q(x)}{P(x)}\\
	&\stackrel{}{\leq} \sum_x P(x)\left(\frac{Q(x)}{P(x)}-1\right)\\
	&=\sum_x Q(x) - \sum_x P(x)\\
	&= 1 - 1\\
	&= 0
	\end{align}
	
	In the (2) step we use the inequality $\ln x \leq x-1$.
\end{proof}


\begin{proof}[proof 2]
	I recall Jensen's inequality in our textbook when I write the first proof. We can use it to get another proof.
	
	\begin{align}
	-D(P||Q)&= \sum_x P(x)\ln \frac{Q(x)}{P(x)}\\
	&\stackrel{}{\leq} \ln\sum_x P(x) \frac{Q(x)}{P(x)}\\
	&=\ln\sum_x Q(x)\\
	&=\ln 1\\
	&=0
	\end{align}
	
	In the (7) step we use Jensen's inequality.
\end{proof}
\section*{Reference}
[1] \url{https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence}

\end{document}